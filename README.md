## Отчет по исследованию Looped Transformers и их возможностей.
Мини исследование-отчет по статье - https://arxiv.org/abs/2311.12424

## Оглавление
- [Введение](#introducion)
- [Сравнение эффективности Looped TFs с обычными TFs](#basic-comparision)
- - [Итеративные свойства](#basic-comparision-iter-props)
- - [Дополнительно](#basic-comparision-additional)
- [N токенов](#n-tokens)
- - [Попробуем брать N последних токенов](#n-last-tokens)
- - [Попробуем брать N первых токенов](#n-first-tokens)
- - [Дополнение](#n-tokens-additive)
- [Looped n-layers](#looped-n-layers)
- - [Основной эксперимент](#looped-n-layers-exp)
- - [Сходимость на этапе тренировки](#looped-n-layers-convergence)
- [Заменим Attention на SSM](#ssm)
- - [Постановка задачи](#ssm-start)
- - [Mamba](#ssm-mamba)
- - [Послесловие](#ssm-mamba-afterword)
- [Немного изменим модель (Bonus)](#lr)
- [Дополнительные наблюдения](#additive)
- [Дискуссия и заключение](#conclusion)

<a id="introducion"></a>
## Введение
За основу исследования взята статья **Looped Transformers are Better at Learning Learning Algorithms**. В данной работе мы посмотрим на возможности по улучшению модели, скорости ее сходимости, проверим несколько гипотез и попробуем обосновать полученный результат. Выведем метрики для сравнения архитектур при различных параметрах и посмотрим что влияет на тот или иной результат. Б<em>о</em>льшая часть эксперементов проводилась с параметром $ b=10 $ и количеством точек $k={31, 41}$. Список модификаций исходного репозитория приведен в конце.

Одна экспериментов проводилась в ноутбуках формата `experiment_*.ipynb`. Другая часть эксперементов проводилась в удаленной среде (kaggle), конфигурации для находятся в папке `configs`.

Задача работы: сравнить возможность зацикленных и обычных моделей для **In-Context Learning** в контексте линейной регрессии (с шумом и без). Наша целевая функция - квадратичная ошибка.  

<a id="basic-comparision"></a>
## Сравнение эффективности Looped TFs с обычными TFs
Все эксперементы проводились с выключенным Mixed Precision, по скольку он не влиял на скорость обучения - видимо из-за того что gpt2_nano не может использовать GPU эффективно, тренировка оставалась относительно медленной.
Одна часть эксперементов проводилась на локальной машине с Nvidia 3060ti mobile, а вторая на удаленной машине с Nvidia Tesla P100.

Для сравнения двух типов моделей, посмотрим на скорость сходимости трансформера (TF) с параметром $L = 12$ и Looped TFs с параметрами $ L = 1; b = \{5, 10, 20, 25\}; T= \{10, 20\}$. Были использованы следующие параметры Heads=4, dims=10, points=31. Потери масштабированы на размерность регрессии.

|                                                  |                                             |
|--------------------------------------------------|---------------------------------------------|
| ![](/images/Compare_losses_b_n_t_10.png "Title") | ![](/images/Compare_losses_b_n.png "Title") |

Как можно видеть из графиков, Looped TFs при увеличении $ b $ дает улучшение метрик. 
При количестве параметров в 12 раз меньшим, чем у обычного трансформера, он показывает неплохие показатели. 
Конфигурации для запуска моделей взяты из папки эксперимента №5.

<a id="basic-comparision-iter-props"></a>
### Итеративные свойства
Сравним точноть моделей и посмотрим на их поведение на разных итерациях (в дальнейшем данный метод будет использовать ни один раз). Для этого обучим Looped TFs со значениями $ b={5, 10, 20}; T={ 20, 10} $  и посмотрим на их сходимость при больших $ b $.

|           |       |
|-----------|-------|
| ![alt text](/images/check_for_convergence_properties.png "Title")    | ![alt text](/images/check_for_scheduling_convergence_properties_noisy_linear_regression_T_10_short.png "Title") |

Из графика можем видеть что при большем $ b $ на тренировке Looped TF показывает себя лучше, при увеличении количества итераций на инференсе. Результаты модели становятся стабильными.  
"Эксперименты проводились в блокноте `experiment_schedule.ipynb`. 

<a id="basic-comparision-additional"></a>
### Дополнительно
Появилась следующая гипотеза:

>**Гипотеза**
>
>Шедулинг может помочь для обобщающих способностей модели. 
>Увеличивая параметр $ b $ во время тренировки можно получить лучшую обобщающую способность.

Однако в процессе эксперимента со слабыми моделями (`n_embs=128`) это подтвердилось только частично. 
При тренировке с разным $ b $, модель дает меньшую ошибку на большем количестве итераций при $ T=20 $, 
однако при $ T=10 $ утверждать то же не можем.  
Опыты проводились в блокноте `experiment_shedule.ipynb` и в блокноте `experiment_shedule_2.ipynb`.

|           |                                                                                            | |
|-----------|--------------------------------------------------------------------------------------------|-|
| ![](/images/check_for_scheduling_convergence_properties.png "Title")    | ![](/images/check_for_scheduling_convergence_properties_train_steps_10000.png "Title")     |![](/images/check_for_scheduling_convergence_properties_noisy_linear_regression_256.png "check_for_scheduling_convergence_properties_noisy_linear_regression_256")   |

Из графика можно видеть, что в отличии от моделей с параметрами $ b=5; b=10; b=15 $, у модели с варьированием $ b $ на этапе тренировки показатели точности сильно разнятся. Также были протестированы модели с размерностью `n_embs=256`, то есть мы увеличели размерность моделей в два раза относительно предыдущих, но это не изменинило положения. Возможно данные результаты связаны с тем, что модель пытается подстроится под разное количество итераций во время обучения. 

<a id="n-tokens"></a>
## N токенов
Проверим гипотезу

> ***Гипотеза***
>
> Модель имеет возможность использовать часть токенов как хранилище информации с предыдущего шага.

Постановка экспериментов. Будем убирать как $n$-первых так и $n$-последних токенов на отдельно обученных моделях с разным $T$.
По оси абсцисс - количество используемых токенов, а по оси ординат - квадратичная ошибка.

<a id="n-last-tokens"></a>
### Попробуем брать N последних токенов
Чтобы понять на сколько мы можем обрезать количество токенов которые подаются модель замаскируем часть из них и посмотрим на метрики.
Модели обучались с параметрами $ b = {5, 10, 20}; T = {10, 20} $ при количестве шагов равным 20000. 

На графиках ниже мы можем видеть вполне ожидаемый результат, для моделей с $ b = \{5, 10, 20\} $ обученных с параметром $ T = \{10, 20\}$ точность ухудшается вместе с уменьшением количества токенов поступающих на вход.

![](/images/check_last_n_tokens_quality.png "check_last_n_tokens_quality")

Также интересно, что при увеличении количества итераций при инференсе мы получаем такую же ошибку на каждом этапе отбрасывания $ n $ токенов.
Можно сделать вывод, что ограничение количества подаваемых с конца токенов сильно влияет на качество, но что если брать $ n $ первых токенов?

<a id="n-first-tokens"></a>
### Попробуем брать N первых токенов
Попробуем маскировать все токены кроме $ n $ первых и посмотрим на метрики модели.

![](images/check_first_n_tokens_quality.png "check_first_n_tokens_quality")

Как можем видеть ощутимой разницы это не дало, видимо для зацикленных трансформеров первые и последние токены имеют схожую важность. 
Меньшее количество токенов негативно влияет на итоговые показатели модели. 
Эксперименты проводились в блокнотах `experiment_last_n_tokens.ipynb` и `experiment_first_n_tokens.ipynb`.

Можно сделать вывод что исходная гипотеза верна, дополнительные
токены на входе действительно позволяют модели обучаться для хранения информации о токенах с предыдущей итерации. 

<a id="n-tokens-additive"></a>
### Дополнение
В результате проведения экспериментов проявилось интересное поведение, которое потенциально может вести к переобучению модели и иногда отсутствию возможности адаптироваться на новые данные. При использовании 20-30 токенов мы на обоих графиках можем наблюдать уменьшение точности и на последней точке перед скачком резкое  увеличение точности. Данное поведние стоит исследовать на разной размерности, количестве токенов и других задачах классификации (дерево, MLP, ... ).

Также при убирании $n$ токенов на этапе обучания алгоритм сходился значительно медленнее или чаще всего не сходился вовсе. Возможно стоит изменить подход к маскированию и посмотреть как модель будет обучаться если какой-то процент данных зашумлен.

<a id="looped-n-layers"></a>
## Looped n-layers.
Проверим возможности зацикленных трансформеров 
при обучении с количеством слоев $ L > 1 $.

<a id="looped-n-layers-exp"></a>
### Основной эксперимент.
Будем обучать
зацикленные трансформеры с вариьрованием 
$b, L, T$. 
На графике ниже можем видеть, что увеличение количества
слоев у модели дает лучшую фиксированную точность 
после запятой при увеличенном количестве итераций.

![](images/experiment_looped_n_layers_convergence.png "experiment_looped_n_layers_convergence")

Данный паттерн повторяется с разным параметром $T$ и при 
разной (случайной) инициализации весов. Предположение состоит в том, что при увеличенном $L$ у модели б<em>о</em>льшая обобщающая способность этим может объясняться как более долгая сходимость на этапе тренировки, так и лучшие значения во время валидации модели.

<a id="looped-n-layers-convergence"></a>
### Сходимость на этапе тренировки
Из интересного, стоит отметить, что, чем больше слоев у модели, тем больше шагов итерации ей необходимо чтобы сойтись. К примеру для моделей где $b=10$ и $L=1$ среднеквадратичная ошибка начинает убывать быстрее к минимальному наблюдаемому результату и выход на плато достигается тоже быстрее чем при $L=2$ и сильно быстрее чем у $L=4$. 
Результаты изображены на графике ниже:

|                                                  |                                             |
|--------------------------------------------------|---------------------------------------------|
| ![](/images/loss_b5_t10.png "loss_b5_t10") | ![](/images/loss_b10_t20.png "loss_b10_t20") |



Получается, что сходимость зависит не только от числа итераций 
$b$, но и от числа слоев $L$. Результаты были получены при
разном параметре $T$.

Эксперимент проводился в блокноте 
`experiment_looped_n_layers.ipynb`.

<a id="ssm"></a>
## Заменим Attention на SSM

<a id="ssm-start"></a>
### Постановка задачи
В данной серии эксперементов попробуем исследовать как ведет себя **Mamba** (архитектура из статьи [**Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality**](https://arxiv.org/pdf/2405.21060) ). Реализация взята из библиотеки `zetascale` и репозитория [**mamba-minimal**](https://github.com/johnma2006/mamba-minimal). Стоит заметить, что время обучения модели сильно больше в отличие стандартного блока трансформера, по скольку там не реализованы многие методы оптимизации. Вместо стандартного диапазона тренировки $8 min - 20 min$ обучение модели занимает $ 1 h - 8 h$.

Проверим следующую гипотезу:

> ***Гипотеза***
>
> Поведение моделей будет похоже как при тренировке, так и при валидации.


<a id="ssm-mamba"></a>
### Mamba
Для построения модели заменим основную модель **GPT-nano** на архитектуру **Mamba**. Сама реализация находится в `mamba_nano.py`. Была обучена как базовая модель с одним слоем так и c несколькими. Были обучены 4 модели c параметрами. Разное количество шагов вызвано большим временем обучения модели, но однако мы все равно можем проследить основные моменты, такие как выход на плато и начало падения целевой метрики. Результаты обучения приведены на графике ниже. Для более наглядного сравнения использовалось скользящее среднее. 

![](images/compare_mamba_models.png "compare_mamba_models")

Как мы можем увидеть модель проявляет себя достаточно интересно, на первых итерациях квадратичная ошибка падает сильно быстрее чем у Looped TF и далее идет выход на плато, но несмотря на эти показатели, итоговая ошибка у моделей на базе архитектуры **Mamba**, больше чем у моделей на базе трансформеров. Посмотрев на график ниже появляется предположение, что **Mamba** накапливает больше ошибок в процессе выполнения (по сравнению с трансформером, который становится более стабильным). 

![](images/pointwise_loss_noise.png "compare_mamba_models")

Интересно что зацикленная **Mamba** показывает себя стабильнее на большем количестве итераций при валидации. Основная гипотеза состоит в том, что сама архитектура модели более натурально работает в цикле. По скольку **Mamba** это sequence-based network, в теории она может более натурально работать с циклами, однако этот тезис стоит проверить основательнее, на других эксперименетах.

![](images/looped_mamba_vs_looped_tf.png "looped_mamba_vs_looped_tf")

<a id="ssm-mamba-afterword"></a>
### Послесловие
Для конечных выводов по использовании данной архитектуры и ее возможности с зацикливанием стоит провести больше эксперементов, но ввиду сильного увеличения времени работы сделать это тяжело, не прибегая к тем улучшениям (одна из самых важных частей статьи) которые делают модели **Mamba** быстрее при увеличении контекста. Однако, изначальная гипотеза не подтвердилась, модель показала себя слабее относительно трансформеров, однако возможный тюнинг параметров может помочь модели достигнуть похожих показателей точности. И хотя модель достигает меньшей точности по сравнению с трансформерами, можно увидеть что ее зацикленная версия ведет себя значительно лучше, а это значит, что и с другими архитектурами, их зацикленные версии могут дать значительный прирост в точности.

<a id="lr"></a>
## Немного изменим модель
По скольку одной из основой архитектуры модели является цикл с находящимся внутри трансформером, и по скольку мы стараемся имитировать итеративные алгоритмы, попробуем позаимствовать идею из градиентного спуска. Дадим модели новый параметр `lr`, который будет обучаться вместе с моделью. Это будет всего один параметр принадлежащий $\mathbb{R}$. Будем каждую итерацию цикла умножать исходные ембединги на `lr`. Это должно дать дополнительную информацию о конкретной итерации относительно цикла. 
Обучим для более справедливого оценивания несколько моделей с параметрами $b=\{5, 10, 15\}$ и модели без модификации на этих же конфигурациях. 

![](images/experiment_lr_looped_tf.png "experiment_lr_looped_tf")

В результате мы получили более точную модель. При больших итерациях, за исключением шедулинга, где результат идентичен, мы можем видеть, что дополнительная информация о цикле помогает уменьшить ошибку после обучения. Разница в квадратичной ошибке составляет ~0.005, ~0.005, ~0.0024 в для $b = \{5, 10, 15\}$ соответственно. 

|               | b=5    | b=10   | b=15    |
|---------------|--------|--------|---------|
| Classic model | 0.0067 | 0.0070 | 0.00328 |
| LR model      | 0.0014 | 0.0019 | 0.00086 |

Посмотрим на график обучения модели и на скользящее среднее во время тренировки. Можем увидеть что наше дополнение к архитектуре позволяет ускорить сходимость алгоритма.

![](images/experiment_lr_looped_tf_train_loss.png "experiment_lr_looped_tf_train_loss")

Эксперимент проводился в блокноте `experiment_bonus.ipynb`.


<a id="additive"></a>
## Дополнительные наблюдения
По скольку Looped TFs имеют преимущество во многих ситуациях над обычными TF с таким же или б<em>о</em>льшим количеством параметров исследование этого поведения может помочь в понимании природы такого поведения. Как один из вариантов можно посмотреть на ландшафт потерь моделей обученных на разных задачах и посмотреть на их различия. Метод взят из статьи [**Visualizing the Loss Landscape of Neural Nets**](https://arxiv.org/pdf/1712.09913). 

Было обучено 4 модели, один зацикленный трансформер со значениями $b = \{5, 10, 15\}; L= 1$ и обычный трансформер со значением $L=\{2\}$. Трансформер с одним слоем не использовался ввиду своей невозможности сойтись. Графики ландшафтов можно увидеть ниже.

![](images/loss_surface.png "loss_surface")
Из интересного можно увидеть, что у Looped TF ландшафт потерь более равномерный чем у обычного трансформера, также в ряде случаев ландшафт более крутые относительно точки минимума, что может свидетельствовать о более скорой сходимости и это же мы можем видеть на графиках обучения модели. Хотя по графикам нельзя однозначно говорить что влияет на такое хорошее поведение у Looped TF, но позволяет дать некоторые идеи того, что может происходить при обучении модели.

Все эксперименты проводились в ноутбуке `loss_surface.ipynb`, 3д графики можно увидеть там же или в папке `images`. 


<a id="conclusion"></a>
## Дискуссия и заключение
Мы провели ряд эксперементов с Looped TF и TF, варьировали их параметры и попытались интерпертировать вывод модели. Можно сказть что зацикленные трансформеры показали свою эффективность в задачах линейной регрессии. Многие выводы сделанные в оригинальой статье подтвердились. 
Было выяснено, что при увеличении числа слоев модели вели себя ожидаемо, улучшая итоговое качество метрик, однако если слоев становилось слишком много, модель сходилась сильно дольше (особенно при увеличении $b$). Чтобы это исправить можно попробовать изменить архитектуру модели, добавить внутренний шедулинг чтобы динамически изменять параметр $b$ на этапе инференса модели или же вовсе выбирать количество итераций динамически. 

Чтобы улучшить обобщающие способности модели можно провести больше экспериментов с варьированием $b$ на этапе тренировки, это может привести к улучшению результата.

Архитектура *Mamba* и ее "*Looped*" версия показала себя многообещающе, и хотя не получилось провести все необходимые эксперименты, ввиду времени тренировки, она показала себя достаточно хорошо, чтобы исследовать на ней задачу **In-Context learning** в контексте моделей Universal Transformer.

Сама же интерпретация моделей вызывает большой интерес. Стоит более внимательно изучить поведение моделей как итеративного алгоритма в задаче **In-Context learning** для более сложных задач таких как языковое моделирование (LM) или же компьютерное зрение (CV).
