{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T06:04:00.230078Z",
     "start_time": "2024-07-25T06:04:00.214395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "###Consts\n",
    "STEPS = 100_000\n",
    "SEED = 42\n",
    "DEVICE = 'cuda'\n",
    "MODEL_N_DIMS = 10\n",
    "LOG_EVERY_STEPS = 100\n",
    "SAVE_EVERY_STEPS = 1000\n",
    "KEEP_EVERY_STEPS = 1000"
   ],
   "id": "1795825b9c0e6126",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T06:05:35.998456Z",
     "start_time": "2024-07-25T06:05:34.880441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Import libraries and scripts\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import wandb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "from tasks import get_task_sampler\n",
    "from models import TransformerModel\n",
    "from main_utils import gen_dataloader\n",
    "from curriculum import CurriculumSimple"
   ],
   "id": "54cbc5a80407b514",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-25T06:04:05.496689Z",
     "start_time": "2024-07-25T06:04:05.067147Z"
    }
   },
   "source": [
    "model = TransformerModel(\n",
    "    n_dims=4,\n",
    "    n_positions=201,\n",
    "    n_embd=256,\n",
    "    n_layer=12,\n",
    "    n_head=8,\n",
    "    pred_type=\"regression\",\n",
    ").cuda()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 9.48M\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T06:04:09.852407Z",
     "start_time": "2024-07-25T06:04:09.394495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "task_sampler = get_task_sampler(\n",
    "    task_name=\"linear_regression\",\n",
    "    batch_size=2,\n",
    "    n_points=7,\n",
    "    n_dims=4,\n",
    "    n_dims_truncated=1,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "real_task = task_sampler()\n",
    "xs, ys = real_task.xs.float(), real_task.ys.float()\n",
    "n_loops = 1; horizon_start = max(0, n_loops - 20)\n",
    "xs, ys, xs.shape, ys.shape"
   ],
   "id": "30ed964d1224d8b3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.3078,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0933,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.4446,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.9205,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.2219,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.8618,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.5656,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.1971,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.6093,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.1747,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.5403,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.6959,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.9754,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.3829,  0.0000,  0.0000,  0.0000]]], device='cuda:0'),\n",
       " tensor([[ 0.5135, -0.0366,  0.1746, -0.3614, -0.0871,  0.3384,  0.6147],\n",
       "         [-0.3464, -1.0705,  2.0641,  2.7065, -1.2228,  1.7138,  0.6729]],\n",
       "        device='cuda:0'),\n",
       " torch.Size([2, 7, 4]),\n",
       " torch.Size([2, 7]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T06:04:17.630896Z",
     "start_time": "2024-07-25T06:04:17.395797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_pred = model(xs, ys, add_inputs_embeds=False) \n",
    "print(y_pred)\n",
    "loss = (ys - y_pred).square().mean()\n",
    "loss"
   ],
   "id": "8ad2dfef24798b7d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6317,  0.0810, -0.3501,  0.4283,  0.1921, -0.5659, -0.6622],\n",
      "        [ 0.1001,  0.3541, -0.6169, -0.6579,  0.4289, -0.5863, -0.2742]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vadim_K\\DataspellProjects\\looped_transformer\\scripts\\nano_gpt.py:79: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  y = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.4573, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T12:50:03.088275Z",
     "start_time": "2024-07-24T12:50:03.018903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def train_step(curriculum, model, xs, ys, optimizer, ctx, scaler, n_loop_window, family = 'gpt2'):\n",
    "    \"\"\"n_loop_window: T\"\"\"\n",
    "    if family == 'gpt2':\n",
    "        with ctx:\n",
    "            #y_pred = model(xs, ys, add_inputs_embeds=args.training.add_inputs_embeds)  # [B, n]\n",
    "            y_pred = model(xs, ys, add_inputs_embeds=False)  # [B, n]\n",
    "            # list of [B, n], length K + 1, get rid of the 0-th one\n",
    "            loss = (ys - y_pred).square().mean()  # auto on both K and n (number of in context samples)\n",
    "            \n",
    "    elif family == 'gpt2_loop':\n",
    "        n_loops = curriculum.n_loops  # K\n",
    "        with ctx:\n",
    "            horizon_start = max(0, n_loops - n_loop_window)\n",
    "            y_pred_list = model(xs, ys, horizon_start, n_loops)\n",
    "            # list of [B, n], length K\n",
    "            y_pred_arr = torch.cat(y_pred_list, dim=0)  # [B * K, n]\n",
    "            y_star_arr = torch.cat([ys] * len(y_pred_list), dim=0)  # [B * K, n]\n",
    "            loss = (y_star_arr - y_pred_arr).square().mean()  # auto on both K and n (number of in context samples)\n",
    "            y_pred = y_pred_list[-1]  # [B, n]\n",
    " \n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    return loss.detach(), y_pred.detach()\n",
    "\n",
    "def train_loop(model, \n",
    "               lr=0.0001, \n",
    "               use_fixed_dataset = False, \n",
    "               task_name = \"linear_regression\", \n",
    "               batch_size=64, \n",
    "               model_n_dims=MODEL_N_DIMS,\n",
    "               train_size = 10000, test_size=1000,\n",
    "               family = \"gpt2\",\n",
    "               experiment_name = \"linear_regression_gpt_2\",):\n",
    "    state_path = \"state.pt\"\n",
    "    # TORCH 2.0 ZONE ###############################\n",
    "    torch.set_float32_matmul_precision('highest')\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul\n",
    "    torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn\n",
    "    dtype = 'float16'  # 'bfloat16', 'float32'\n",
    "    ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "    ctx = torch.amp.autocast(device_type='cuda', dtype=ptdtype, cache_enabled=False)\n",
    "\n",
    "    ################################################\n",
    "    \n",
    "    wandb.init(\n",
    "        dir=\"results\",\n",
    "        project=\"looped_transformers\",\n",
    "        #config=args.__dict__,\n",
    "        notes=\"args.wandb.notes\",\n",
    "        name=experiment_name,\n",
    "        mode=\"disabled\",\n",
    "        resume=True,\n",
    "    )\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    # model = torch.compile(model)\n",
    "\n",
    "    model.to(DEVICE)\n",
    "    model.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "    curriculum = CurriculumSimple(dims_start = 5,\n",
    "        points_start = 11,\n",
    "        loops_start= 1,\n",
    "        dims_schedule = 0,\n",
    "        points_schedule = 0,\n",
    "        loops_schedule = 0)\n",
    "\n",
    "    # Here the model load the pretrained model\n",
    "    # args, model, optimizer, curriculum, state_path, starting_step = load_pretrained_model(\n",
    "    #     args, model, optimizer, curriculum, device)\n",
    "\n",
    "    if use_fixed_dataset:\n",
    "        from main_utils import gen_dataloader\n",
    "        task_sampler = get_task_sampler(\n",
    "            task_name=task_name,\n",
    "            batch_size=batch_size,\n",
    "            n_points=curriculum.n_points,\n",
    "            n_dims=model_n_dims,\n",
    "            n_dims_truncated=curriculum.n_dims_truncated,\n",
    "            device=DEVICE,\n",
    "            sparsity=False,\n",
    "        )\n",
    "        train_loader = gen_dataloader(task_sampler, train_size, batch_size)\n",
    "        train_iter = iter(train_loader)\n",
    "        test_loader = gen_dataloader(task_sampler, test_size, batch_size)\n",
    "\n",
    "    pbar = tqdm(range(0, STEPS))\n",
    "    for i in pbar:\n",
    "        if use_fixed_dataset:\n",
    "            try:\n",
    "                batch = next(train_iter)\n",
    "                xs, ys = batch['x'].to(DEVICE), batch['y'].to(DEVICE)\n",
    "            except StopIteration:\n",
    "                train_iter = iter(train_loader)\n",
    "        else:\n",
    "            task_sampler = get_task_sampler(\n",
    "                task_name=task_name,\n",
    "                batch_size=batch_size,\n",
    "                n_points=curriculum.n_points,\n",
    "                n_dims= model_n_dims,\n",
    "                n_dims_truncated= curriculum.n_dims_truncated,\n",
    "                device=DEVICE,\n",
    "                sparsity=False,\n",
    "            )\n",
    "\n",
    "            real_task = task_sampler()\n",
    "            xs, ys = real_task.xs.float(), real_task.ys.float()\n",
    "\n",
    "        loss, output, total_norm, grad_norm_dict = train_step(curriculum, model, xs, ys, optimizer, ctx, scaler)\n",
    "        train_loss = loss\n",
    "        \n",
    "        # EVALUATION ======================================\n",
    "        point_wise_tags = list(range(curriculum.n_points))  # [0, 1, 2, ..., n-1]\n",
    "        if i % LOG_EVERY_STEPS == 0:\n",
    "            point_wise_loss = (output - ys).square().mean(dim=0)  # [n,]\n",
    "            if use_fixed_dataset:\n",
    "                # eval\n",
    "                with torch.no_grad():\n",
    "                    for batch in test_loader:\n",
    "                        xs, ys = batch['x'].to(DEVICE), batch['y'].to(DEVICE)\n",
    "                        if family == 'gpt2':\n",
    "                            output = model(xs, ys)  # [B,]\n",
    "                        elif family == 'gpt2_loop':\n",
    "                            n_loops = curriculum.n_loops  # K\n",
    "                            y_pred_list = model(xs, ys, 0, n_loops)\n",
    "                            output = y_pred_list[-1]  # [B, n]\n",
    "                        else:\n",
    "                            raise NotImplementedError\n",
    "                        point_wise_loss = (output - ys).square().mean(dim=0)\n",
    "                        loss = point_wise_loss.mean()\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"overall_loss\": loss,\n",
    "                    \"overall_train_loss\": train_loss,\n",
    "                    \"loop_times\": curriculum.n_loops,\n",
    "                    \"grad_norm/layerwise\": grad_norm_dict,\n",
    "                    \"grad_norm\": total_norm,\n",
    "                    \"pointwise/loss\": dict(\n",
    "                        zip(point_wise_tags, point_wise_loss.detach().cpu().numpy())\n",
    "                    ),\n",
    "                    \"n_points\": curriculum.n_points,\n",
    "                    \"n_dims\": curriculum.n_dims_truncated,\n",
    "                    \"lr\": optimizer.param_groups[0]['lr'],\n",
    "                },\n",
    "                step=i,\n",
    "            )\n",
    "\n",
    "        curriculum.update()\n",
    "\n",
    "        pbar.set_description(f\"loss {loss}\")\n",
    "        if i % SAVE_EVERY_STEPS == 0:\n",
    "            training_state = {\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"train_step\": i,\n",
    "                \"val_loss\": loss,\n",
    "            }\n",
    "            torch.save(training_state, state_path)\n",
    "        if (KEEP_EVERY_STEPS > 0\n",
    "                and i % KEEP_EVERY_STEPS == 0\n",
    "                and i > 0 ) or (i == STEPS - 1):\n",
    "            torch.save({'model': model.state_dict()},\n",
    "                       os.path.join(args.out_dir, f\"model_{i}.pt\"))"
   ],
   "id": "cab8d13d98c0d95c",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MODEL_N_DIMS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[75], line 43\u001B[0m\n\u001B[0;32m     39\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad(set_to_none\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     40\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss\u001B[38;5;241m.\u001B[39mdetach(), y_pred\u001B[38;5;241m.\u001B[39mdetach()\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain_loop\u001B[39m(model, \n\u001B[1;32m---> 43\u001B[0m                lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.0001\u001B[39m, use_fixed_dataset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m, task_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlinear_regression\u001B[39m\u001B[38;5;124m\"\u001B[39m, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m64\u001B[39m, model_n_dims\u001B[38;5;241m=\u001B[39m\u001B[43mMODEL_N_DIMS\u001B[49m):\n\u001B[0;32m     44\u001B[0m     state_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstate.pt\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     45\u001B[0m     \u001B[38;5;66;03m# TORCH 2.0 ZONE ###############################\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'MODEL_N_DIMS' is not defined"
     ]
    }
   ],
   "execution_count": 75
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
