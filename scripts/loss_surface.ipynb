{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-02T09:57:24.044713Z",
     "start_time": "2024-08-02T09:57:24.034530Z"
    }
   },
   "source": [
    "import torch\n",
    "import losslandscape as land\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models import TransformerModelLooped, TransformerModelLoopedPyHessianWrapper, TransformerModelPyHessianWrapper\n",
    "from curriculum import CurriculumSimple\n",
    "from train import train_without_config, validate_model\n",
    "from scripts.tasks import get_task_sampler\n",
    "\n",
    "n_dims = 10\n",
    "train_steps = 15000\n",
    "log_every = 500\n",
    "device = \"cuda:0\""
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T09:57:24.075257Z",
     "start_time": "2024-08-02T09:57:24.062723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "task_sampler = get_task_sampler(\n",
    "    task_name=\"linear_regression\",\n",
    "    batch_size=1,\n",
    "    n_points=31,\n",
    "    n_dims=10,\n",
    "    n_dims_truncated=10,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "real_task = task_sampler()\n",
    "xs, ys = real_task.xs.float(), real_task.ys.float()"
   ],
   "id": "a76c2fab890e6005",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T09:57:24.106989Z",
     "start_time": "2024-08-02T09:57:24.097001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## criterion function for loss plot calculation \n",
    "def criterion_fn(y_pred_list, ys_in):\n",
    "    y_pred_arr = torch.cat(y_pred_list, dim=0)  # [B * K, n]\n",
    "    y_star_arr = torch.cat([ys_in] * len(y_pred_list), dim=0)  # [B * K, n]\n",
    "    return (y_star_arr - y_pred_arr).square().mean()"
   ],
   "id": "192ed27ca23ada16",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T10:05:06.168388Z",
     "start_time": "2024-08-02T09:57:24.109436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_loop_b5 = TransformerModelLoopedPyHessianWrapper(\n",
    "    n_dims=n_dims,\n",
    "    n_positions=101,\n",
    "    n_embd=128,\n",
    "    n_layer=1,\n",
    "    n_head=4,\n",
    "    pred_type=\"regression\",\n",
    "    default_n_loops=5\n",
    ").to(device)\n",
    "\n",
    "model_b5_config = {\n",
    "    \"curriculum\" : CurriculumSimple(n_dims, 31, 5, [5000, n_dims, 0], [5000, 31, 0], [1000, 5, 0]),\n",
    "    \"log_steps\" : train_steps // log_every,\n",
    "    \"params\" : [],\n",
    "    \"losses\" : [],\n",
    "    \"metrics\" : None\n",
    "}\n",
    "\n",
    "def callback_b5_fn(model, loss):\n",
    "    model_b5_config[\"params\"].append(land.ParamList(land.get_params(model)))\n",
    "    model_b5_config[\"losses\"].append(loss)\n",
    "\n",
    "\n",
    "model_b5_config[\"metrics\"] = train_without_config(\n",
    "    model_loop_b5, model_b5_config[\"curriculum\"], model_n_dims=n_dims,\n",
    "    log_every_steps=log_every, train_steps=train_steps, family=\"gpt2_loop\",\n",
    "    do_wandb_log=False, seed=None, task_name=\"linear_regression\", callback=callback_b5_fn)"
   ],
   "id": "c2758cf575a46f74",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.20M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 4.830509662628174:  85%|████████▍ | 12677/15000 [07:41<01:24, 27.48it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[23], line 24\u001B[0m\n\u001B[0;32m     20\u001B[0m     model_b5_config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparams\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(land\u001B[38;5;241m.\u001B[39mParamList(land\u001B[38;5;241m.\u001B[39mget_params(model)))\n\u001B[0;32m     21\u001B[0m     model_b5_config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlosses\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(loss)\n\u001B[1;32m---> 24\u001B[0m model_b5_config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetrics\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_without_config\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     25\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_loop_b5\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_b5_config\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcurriculum\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_n_dims\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_dims\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     26\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlog_every_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlog_every\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_steps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfamily\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgpt2_loop\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     27\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdo_wandb_log\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtask_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlinear_regression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallback_b5_fn\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\DataspellProjects\\looped_transformer\\scripts\\train_core.py:187\u001B[0m, in \u001B[0;36mtrain_without_config\u001B[1;34m(model, curriculum, lr, add_inputs_embeds, task_name, batch_size, n_loop_window, model_n_dims, train_steps, family, experiment_name, out_dir, do_wandb_log, log_every_steps, use_ctx, project_name, project_notes, seed, weight_decay, sparsity, save_every_steps, device, callback)\u001B[0m\n\u001B[0;32m    184\u001B[0m real_task \u001B[38;5;241m=\u001B[39m task_sampler()\n\u001B[0;32m    185\u001B[0m xs, ys \u001B[38;5;241m=\u001B[39m real_task\u001B[38;5;241m.\u001B[39mxs\u001B[38;5;241m.\u001B[39mfloat(), real_task\u001B[38;5;241m.\u001B[39mys\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[1;32m--> 187\u001B[0m loss, output, total_norm, grad_norm_dict \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcurriculum\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mxs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mys\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscaler\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    188\u001B[0m \u001B[43m                                                      \u001B[49m\u001B[43madd_inputs_embeds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfamily\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfamily\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_ctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_ctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_loop_window\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_loop_window\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    190\u001B[0m \u001B[38;5;66;03m# EVALUATION ======================================\u001B[39;00m\n\u001B[0;32m    191\u001B[0m point_wise_tags \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mrange\u001B[39m(curriculum\u001B[38;5;241m.\u001B[39mn_points))  \u001B[38;5;66;03m# [0, 1, 2, ..., n-1]\u001B[39;00m\n",
      "File \u001B[1;32m~\\DataspellProjects\\looped_transformer\\scripts\\train_core.py:105\u001B[0m, in \u001B[0;36mtrain_step\u001B[1;34m(curriculum, model, xs, ys, optimizer, ctx, scaler, add_inputs_embeds, use_ctx, n_loop_window, family)\u001B[0m\n\u001B[0;32m    103\u001B[0m     scaler\u001B[38;5;241m.\u001B[39mupdate()\n\u001B[0;32m    104\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 105\u001B[0m     \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    106\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m    107\u001B[0m norm_dict, total_norm \u001B[38;5;241m=\u001B[39m calculate_gradient_norm(model)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\looped_tf\\lib\\site-packages\\torch\\_tensor.py:525\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    515\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    517\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    518\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    523\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    524\u001B[0m     )\n\u001B[1;32m--> 525\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    526\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    527\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\looped_tf\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    262\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    264\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    265\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    266\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 267\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    274\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    275\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\looped_tf\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    742\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    743\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 744\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    745\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    746\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    747\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    748\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_b5_config[\"losses\"] = torch.tensor(model_b5_config[\"losses\"]).to(device)\n",
    "\n",
    "loss_landscape = land.LossLandscapePlotting(\n",
    "    model=model_loop_b5,\n",
    "    criterion= criterion_fn,\n",
    "    device=device,\n",
    "    data=(torch.concatenate([xs, ys.unsqueeze(-1)], dim=-1), ys),\n",
    "    parameters_history=model_b5_config[\"params\"],\n",
    "    loss_history=model_b5_config[\"losses\"],\n",
    "    mean_theta0=True\n",
    ")"
   ],
   "id": "1ea557e4eb27759d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trace = loss_landscape.compute_trace(every_ith=1)\n",
    "ralpha, rbeta, surface = loss_landscape.compute_landscape(trace, arange=(-1, 1), brange=(-1, 1), grid_density=20, coef=1)"
   ],
   "id": "26a392f2ed0a3139",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "loss_landscape.plot(\n",
    "    trace=trace, # [trace[0], trace[-1]],\n",
    "    ralpha=ralpha, rbeta=rbeta, surface=surface,\n",
    "    colormap=\"magma\", k=0.5\n",
    ")"
   ],
   "id": "66193607841910db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "loss_landscape.plot_contour(\n",
    "    trace=trace,\n",
    "    ralpha=ralpha, rbeta=rbeta, surface=surface,\n",
    "    colormap=\"magma\", k=1\n",
    ")"
   ],
   "id": "124dabc935c76f7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_l2 = TransformerModelPyHessianWrapper(\n",
    "    n_dims=n_dims,\n",
    "    n_positions=101,\n",
    "    n_embd=128,\n",
    "    n_layer=2,\n",
    "    n_head=4,\n",
    "    pred_type=\"regression\"\n",
    ").to(device)\n",
    "\n",
    "model_l2_config = {\n",
    "    \"curriculum\" : CurriculumSimple(n_dims, 31, 0, [5000, n_dims, 0], [5000, 31, 0], [1000, 0, 0]),\n",
    "    \"log_steps\" : train_steps // log_every,\n",
    "    \"params\" : [],\n",
    "    \"losses\" : [],\n",
    "    \"metrics\" : None\n",
    "}\n",
    "\n",
    "def callback_l2_fn(model, loss):\n",
    "    model_l2_config[\"params\"].append(land.ParamList(land.get_params(model)))\n",
    "    model_l2_config[\"losses\"].append(loss)\n",
    "\n",
    "model_l2_config[\"metrics\"] = train_without_config(\n",
    "    model_l2, model_l2_config[\"curriculum\"], model_n_dims=n_dims,\n",
    "    log_every_steps=log_every, train_steps=train_steps, family=\"gpt2\",\n",
    "    do_wandb_log=False, seed=None, task_name=\"linear_regression\", callback=callback_l2_fn)"
   ],
   "id": "5e7f37f1d0e7311c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_l2_config[\"losses\"] = torch.tensor(model_l2_config[\"losses\"]).to(device)\n",
    "\n",
    "def criterion_fn(y_pred, ys):\n",
    "    return (ys - y_pred).square().mean()\n",
    "\n",
    "\n",
    "from scripts.tasks import get_task_sampler\n",
    "\n",
    "task_sampler = get_task_sampler(\n",
    "    task_name=\"noisy_linear_regression\",\n",
    "    batch_size=1,\n",
    "    n_points=31,\n",
    "    n_dims=10,\n",
    "    n_dims_truncated=10,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "real_task = task_sampler()\n",
    "xs, ys = real_task.xs.float(), real_task.ys.float()\n",
    "\n",
    "loss_landscape = land.LossLandscapePlotting(\n",
    "    model=model_l2,\n",
    "    criterion=criterion_fn,\n",
    "    device=device,\n",
    "    data=(torch.concatenate([xs, ys.unsqueeze(-1)], dim=-1), ys),\n",
    "    parameters_history=model_l2_config[\"params\"],\n",
    "    loss_history=model_l2_config[\"losses\"],\n",
    "    mean_theta0=True,\n",
    ")\n",
    "  "
   ],
   "id": "7ca716dc9017909",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trace = loss_landscape.compute_trace(every_ith=1)\n",
    "ralpha, rbeta, surface = loss_landscape.compute_landscape(trace, arange=(-1, 1), brange=(-1, 1), grid_density=20, coef=1)"
   ],
   "id": "d561d125fec85de6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "loss_landscape.plot(\n",
    "    trace=trace,  # [trace[0], trace[-1]],\n",
    "    ralpha=ralpha, rbeta=rbeta, surface=surface,\n",
    "    colormap=\"magma\", k=0.5\n",
    ")"
   ],
   "id": "b467d977a927c0ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "loss_landscape.plot_contour(\n",
    "    trace=trace,\n",
    "    ralpha=ralpha, rbeta=rbeta, surface=surface,\n",
    "    colormap=\"magma\", k=1\n",
    ")"
   ],
   "id": "acbce0e1e7f3241e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
